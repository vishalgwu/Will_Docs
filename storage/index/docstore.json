{"docstore/metadata": {"be4a12fa-331b-48e5-a56b-96b79afd4872": {"doc_hash": "e819dcd794806e3baf440c6500dd46145ca139690a7b71f31772b2cc8ac251f5"}, "362318f7-d5e2-4c25-a6fd-b635e22ca1e3": {"doc_hash": "88c2e6c9f8216af31f744621239486f33d1b65fdca6eae8fd68ccbd085ad9b96"}, "55dab2a4-db23-477e-b9d2-75b03f270a39": {"doc_hash": "562339c2deea81890483c4c0821c9a3ba445f016ca090f501252005ac5b5a9f0"}, "32b065dd-606e-4379-804b-36bb365da6c0": {"doc_hash": "c787136f2a66c1a49286ed38546eed65bdcf1e5ae543db2b7184df9ce31664ad"}, "6bd63a9c-9ede-418e-ba67-49f22529d333": {"doc_hash": "a6055a76ed20a3ff1a9d5a7b7ee78743efb462f533b8eed639c6b8a00e0076aa"}, "90ef2041-ab48-47b1-8685-e4fcaf13ed38": {"doc_hash": "d76304383d8acce615fffa1aeec259aaf7be28240261023acf415102b2521e98", "ref_doc_id": "be4a12fa-331b-48e5-a56b-96b79afd4872"}, "1efb3de1-b9f2-4610-8044-f5815bc7fdf4": {"doc_hash": "10b2adec303238a1877c69627feb3ade058665985163d585696cc9ceb2b736d4", "ref_doc_id": "be4a12fa-331b-48e5-a56b-96b79afd4872"}, "53378685-1d1f-4f1c-b60c-da86faec1153": {"doc_hash": "bfddac4dad9f9705c16e37e87a10788cf658a32e6efb830c75e8035e6b5b24c4", "ref_doc_id": "362318f7-d5e2-4c25-a6fd-b635e22ca1e3"}, "ae41c59e-3517-457f-b486-7bb81b7dbd67": {"doc_hash": "042765d8834ab01d328f506a8f04e29d09ccb4c4bcfd15afb60e92aad4472afc", "ref_doc_id": "362318f7-d5e2-4c25-a6fd-b635e22ca1e3"}, "fda44cce-f1f8-49f5-80e4-420902d6f118": {"doc_hash": "8de6068f500373abcfda901a51a18d8a47fdb69f91cbe1aee34ee3bf52b97fe3", "ref_doc_id": "55dab2a4-db23-477e-b9d2-75b03f270a39"}, "8e48c1b3-8b9e-4d3b-9309-219cf41fbe98": {"doc_hash": "08a980583e835d974becc3dd9fa086b30d87f301c9f0d9d73b4cccba7487aefe", "ref_doc_id": "55dab2a4-db23-477e-b9d2-75b03f270a39"}, "3d04a8e2-5779-4e18-b079-b4f8f8ef84db": {"doc_hash": "d98b0a6df9b00066a0e803e32c83dcf00b7d540fc17a4618d2be0cbc7ef6de6c", "ref_doc_id": "32b065dd-606e-4379-804b-36bb365da6c0"}, "47500594-559c-42e5-9ccd-6f1a714acc84": {"doc_hash": "4636377d0b5fb7efebb8dc2e3d45f0e09d692bb88660c43224123ddc59878732", "ref_doc_id": "32b065dd-606e-4379-804b-36bb365da6c0"}, "b8221cb0-6595-4da5-89c3-ea10bbc7ebb9": {"doc_hash": "09b74506356f857871e3dc9c0a539d43db44770ba3cba4d542626cf3d1834bba", "ref_doc_id": "6bd63a9c-9ede-418e-ba67-49f22529d333"}, "4d5d3f5d-5663-4b34-b8ae-6e661a807388": {"doc_hash": "5afdd16a2f45f4923744e94e2c1adfe9d447ce10a65bb807cafd0b14d37cfe33", "ref_doc_id": "6bd63a9c-9ede-418e-ba67-49f22529d333"}, "9ed5e9ae-e2f6-46d3-8637-3ff86951828e": {"doc_hash": "ccc877c255ec325e027e02fd37627adcfa7f491a8ce102eee37cb3b143037d6b", "ref_doc_id": "6bd63a9c-9ede-418e-ba67-49f22529d333"}}, "docstore/ref_doc_info": {"be4a12fa-331b-48e5-a56b-96b79afd4872": {"node_ids": ["90ef2041-ab48-47b1-8685-e4fcaf13ed38", "1efb3de1-b9f2-4610-8044-f5815bc7fdf4"], "metadata": {"page_label": "1", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}}, "362318f7-d5e2-4c25-a6fd-b635e22ca1e3": {"node_ids": ["53378685-1d1f-4f1c-b60c-da86faec1153", "ae41c59e-3517-457f-b486-7bb81b7dbd67"], "metadata": {"page_label": "2", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}}, "55dab2a4-db23-477e-b9d2-75b03f270a39": {"node_ids": ["fda44cce-f1f8-49f5-80e4-420902d6f118", "8e48c1b3-8b9e-4d3b-9309-219cf41fbe98"], "metadata": {"page_label": "3", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}}, "32b065dd-606e-4379-804b-36bb365da6c0": {"node_ids": ["3d04a8e2-5779-4e18-b079-b4f8f8ef84db", "47500594-559c-42e5-9ccd-6f1a714acc84"], "metadata": {"page_label": "4", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}}, "6bd63a9c-9ede-418e-ba67-49f22529d333": {"node_ids": ["b8221cb0-6595-4da5-89c3-ea10bbc7ebb9", "4d5d3f5d-5663-4b34-b8ae-6e661a807388", "9ed5e9ae-e2f6-46d3-8637-3ff86951828e"], "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}}}, "docstore/data": {"90ef2041-ab48-47b1-8685-e4fcaf13ed38": {"__data__": {"id_": "90ef2041-ab48-47b1-8685-e4fcaf13ed38", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be4a12fa-331b-48e5-a56b-96b79afd4872", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "e819dcd794806e3baf440c6500dd46145ca139690a7b71f31772b2cc8ac251f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1efb3de1-b9f2-4610-8044-f5815bc7fdf4", "node_type": "1", "metadata": {}, "hash": "bccbdbdc94ec4ee93072477586abf650fadd636bdff5274573c785589dfd9355", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\nCem Subakan1, Mirco Ravanelli1, Samuele Cornell2, Mirko Bronzi1, Jianyuan Zhong3\n1Mila-Quebec AI Institute, Canada,\n2Universit`a Politecnica delle Marche, Italy\n3University of Rochester, USA\nABSTRACT\nRecurrent Neural Networks (RNNs) have long been the dominant\narchitecture in sequence-to-sequence learning. RNNs, however, are\ninherently sequential models that do not allow parallelization of their\ncomputations. Transformers are emerging as a natural alternative to\nstandard RNNs, replacing recurrent computations with a multi-head\nattention mechanism.\nIn this paper, we propose the SepFormer, a novel RNN-free\nTransformer-based neural network for speech separation. The Sep-\nFormer learns short and long-term dependencies with a multi-scale\napproach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\ndatasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\nSI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the\nparallelization advantages of Transformers and achieves a compet-\nitive performance even when downsampling the encoded represen-\ntation by a factor of 8. It is thus signi\ufb01cantly faster and it is less\nmemory-demanding than the latest speech separation systems with\ncomparable performance.\nIndex Terms\u2014 speech separation, source separation, trans-\nformer, attention, deep learning.\n1. INTRODUCTION\nRNNs are a crucial component of modern audio processing sys-\ntems and they are used in many different domains, including speech\nrecognition, synthesis, enhancement, and separation, just to name a\nfew. Especially when coupled with multiplicative gate mechanisms\n(like LSTM [1] and GRU [2, 3]), their recurrent connections are es-\nsential to learn long-term dependencies and properly manage speech\ncontexts. Nevertheless, the inherently sequential nature of RNNs\nimpairs an effective parallelization of the computations. This bot-\ntleneck is particularly evident when processing large datasets with\nlong sequences. On the other hand, Transformers [4] completely\navoid this bottleneck by eliminating recurrence and replacing it with\na fully attention-based mechanism. By attending to the whole se-\nquence at once, a direct connection can be established between dis-\ntant elements allowing Transformers to learn long-term dependen-\ncies more easily [5]. For this reason, Transformers are gaining con-\nsiderable popularity for speech processing and recently showed com-\npetitive performance in speech recognition [6], synthesis [7], en-\nhancement [8], diarization [9], as well as speaker recognition [10].\nLittle research has been done so far on Transformer-based mod-\nels for monaural audio source separation. The \ufb01eld has been revo-\nlutionized by the adoption of deep learning techniques [11\u201316], and\nwith recent works [17\u201323] achieving impressive results by adopt-\ning an end-to-end approach. Most of the current speech separation\ntechniques [14, 15, 17\u201322] require effective modeling of long input\nx Encoder h Masking Net Decoder\n\u02c6s1\n\u02c6s2\nm1\nm2\nFig. 1. The high-level description of our system: The encoder block\nestimates a learned-representation for the input signal, while the\nmasking network estimates optimal masks to separate the sources\npresent in the mixtures. The decoder \ufb01nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis \ufb01l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4041, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1efb3de1-b9f2-4610-8044-f5815bc7fdf4": {"__data__": {"id_": "1efb3de1-b9f2-4610-8044-f5815bc7fdf4", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be4a12fa-331b-48e5-a56b-96b79afd4872", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "e819dcd794806e3baf440c6500dd46145ca139690a7b71f31772b2cc8ac251f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90ef2041-ab48-47b1-8685-e4fcaf13ed38", "node_type": "1", "metadata": {"page_label": "1", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "d76304383d8acce615fffa1aeec259aaf7be28240261023acf415102b2521e98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The decoder \ufb01nally reconstructs the esti-\nmated sources in the time domain using the masks provided by the\nmasking network.\nsequences to perform well. Current systems rely, in large part, on the\nlearned-domain masking strategy popularized by Conv-TasNet [15].\nIn this framework, an overcomplete set of analysis and synthesis \ufb01l-\nters is learned directly from the data, and separation is performed\nby estimating a mask for each source in this learned-domain. Build-\ning on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\nbetter long-term modeling is crucial to improve the separation per-\nformance. This is achieved by splitting the input sequence into mul-\ntiple chunks that are processed locally and globally with different\nRNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers\nfrom the aforementioned limitations of recurrent connections, espe-\ncially regarding the global processing step. An attempt to integrate\ntransformers into the speech separation pipeline has been recently\ndone in [22] where the proposed Dual-Path Transformer Network\n(DPTNet) is shown to outperform the standard DPRNN. Such an ar-\nchitecture, however, still embeds an RNN, effectively negating the\nparallelization capability of pure-attention models.\nIn this paper, we propose a novel model called SepFormer (Sep-\naration Transformer), which is mainly composed of multi-head at-\ntention and feed-forward layers. We adopt the dual-path frame-\nwork introduced by DPRNN and we replace RNNs with a multi-\nscale pipeline composed of transformers that learn both short and\nlong-term dependencies. The dual-path framework enables to miti-\ngate the quadratic complexity of transformers, as transformers in the\ndual-path framework process smaller chunks.\nTo the best of our knowledge, this is the \ufb01rst work showing\nthat we can obtain state-of-the-art performance in separation with an\nRNN-free Transformer-based architecture. The SepFormer achieves\nan SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset. It also\nachieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-\n3mix dataset. The SepFormer not only processes all the time steps in\nparallel but also achieves competitive performance when downsam-\npling the encoded representation by a factor of 8. This makes the\nproposed architecture signi\ufb01cantly faster and less memory demand-\n21978-1-7281-7605-5/21/$31.00 \u00a92021 IEEE ICASSP 2021\nICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) | 978-1-7281-7605-5/20/$31.00 \u00a92021 IEEE | DOI: 10.1109/ICASSP39728.2021.9413901\nAuthorized licensed use limited to: The George Washington University. Downloaded on October 09,2025 at 02:25:25 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 3304, "end_char_idx": 6021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "53378685-1d1f-4f1c-b60c-da86faec1153": {"__data__": {"id_": "53378685-1d1f-4f1c-b60c-da86faec1153", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "362318f7-d5e2-4c25-a6fd-b635e22ca1e3", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "88c2e6c9f8216af31f744621239486f33d1b65fdca6eae8fd68ccbd085ad9b96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae41c59e-3517-457f-b486-7bb81b7dbd67", "node_type": "1", "metadata": {}, "hash": "5886ff5e3e5e6d51194b8972022992d73f151caf6d68da1317863552f3bdd885", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ing than the latest RNN-based separation models.\n2. THE MODEL\nThe proposed model is based on the learned-domain masking ap-\nproach [14, 15, 17\u201322] and employs an encoder, a decoder, and a\nmasking network, as shown in Figure 1. The encoder is fully con-\nvolutional, while the masking network employs two Transformers\nembedded inside the dual-path processing block proposed in [17].\nThe decoder \ufb01nally reconstructs the separated signals in the time\ndomain by using the masks predicted by the masking network. To\nfoster reproducibility, the SepFormer will be made available within\nthe SpeechBrain toolkit1.\n2.1. Encoder\nThe encoder takes in the time-domain mixture-signal x \u2208 RT as\ninput, which contains audio from multiple speakers. It learns an\nSTFT-like representation h \u2208RF\u00d7T\u2032\nusing a single convolutional\nlayer:\nh = ReLU(conv1d(x)). (1)\nAs we will describe in Sec. 4, the stride factor of this convolution\nimpacts signi\ufb01cantly on the performance, speed, and memory of the\nmodel.\n2.2. Masking Network\nFigure 2 (top) shows the detailed architecture of the masking net-\nwork (Masking Net). The masking network is fed by the encoded\nrepresentations h \u2208RF\u00d7T\u2032\nand estimates a mask {m1, . . . , mNs }\nfor each of the Ns speakers in the mixture.\nAs in [15], the encoded inputh is normalized with layer normal-\nization [24] and processed by a linear layer (with dimensionalityF).\nWe then create overlapping chunks of size C by chopping up h on\nthe time axis with an overlap factor of 50%. We denote the output of\nthe chunking operation with h\u2032\u2208RF\u00d7C\u00d7Nc , where C is the length\nof each chunk, and Nc is the resulting number of chunks.\nThe representation h\u2032feeds the SepFormer block, which is the\nmain component of the masking network. This block, which will be\ndescribed in detail in Sec. 2.3, employs a pipeline composed of two\ntransformers able to learn short and long-term dependencies.\nThe output of the SepFormer h\u2032\u2032 \u2208RF\u00d7C\u00d7Nc is processed by\nPReLU activations followed by a linear layer. We denote the output\nof this module h\u2032\u2032\u2032\u2208R(F\u00d7Ns)\u00d7C\u00d7Nc , where Ns is the number of\nspeakers. Afterwards we apply the overlap-add scheme described\nin [17] and obtain h\u2032\u2032\u2032\u2032 \u2208RF\u00d7Ns\u00d7T\u2032\n. We pass this representation\nthrough two feed-forward layers and a ReLU activation at the end to\n\ufb01nally obtain the mask mk for each of the speakers.\n2.3. SepFormer Block\nFigure 2 (Middle) shows the architecture of the SepFormer block.\nThe SepFormer block is designed to model both short and long-\nterm dependencies with the dual-scale approach of DPRNNs [17].\nIn our model, the transformer block which models the short-term\ndependencies is named IntraTransformer (IntraT), and the block for\nlonger-term dependencies is named InterTransformer (InterT). In-\ntraT processes the second dimension of h\u2032, and thus acts on each\nchunk independently, modeling the short-term dependencies within\n1speechbrain.github.io/\neach chunk. Next, we permute the last two dimensions (which we\ndenote with P), and the InterT is applied to model the transitions\nacross chunks. This scheme enables effective modelling of long-\nterm dependencies across the chunks. The overall transformation of\nthe SepFormer is therefore de\ufb01ned as follows:\nh\u2032\u2032= finter(P(fintra(h\u2032))), (2)\nwhere we denote the IntraT and InterT with finter(.), and fintra(.),\nrespectively. The overall SepFormer block is repeated N times.\n2.3.1. Intra and Inter Transformers\nFigure 2 (Bottom) shows the architecture of the Transformers used\nfor both the IntraT and InterT blocks. It closely resembles the orig-\ninal one de\ufb01ned in [4]. We use the variable z to denote the input\nto the Transformer. First of all, sinusoidal positional encoding e is\nadded to the input z, such that,\nz\u2032= z + e. (3)\nPositional encoding injects information on the order of the various\nelements composing the sequence, thus improving the separation\nperformance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3845, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae41c59e-3517-457f-b486-7bb81b7dbd67": {"__data__": {"id_": "ae41c59e-3517-457f-b486-7bb81b7dbd67", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "362318f7-d5e2-4c25-a6fd-b635e22ca1e3", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "88c2e6c9f8216af31f744621239486f33d1b65fdca6eae8fd68ccbd085ad9b96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53378685-1d1f-4f1c-b60c-da86faec1153", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "bfddac4dad9f9705c16e37e87a10788cf658a32e6efb830c75e8035e6b5b24c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The overall transformation of\nthe SepFormer is therefore de\ufb01ned as follows:\nh\u2032\u2032= finter(P(fintra(h\u2032))), (2)\nwhere we denote the IntraT and InterT with finter(.), and fintra(.),\nrespectively. The overall SepFormer block is repeated N times.\n2.3.1. Intra and Inter Transformers\nFigure 2 (Bottom) shows the architecture of the Transformers used\nfor both the IntraT and InterT blocks. It closely resembles the orig-\ninal one de\ufb01ned in [4]. We use the variable z to denote the input\nto the Transformer. First of all, sinusoidal positional encoding e is\nadded to the input z, such that,\nz\u2032= z + e. (3)\nPositional encoding injects information on the order of the various\nelements composing the sequence, thus improving the separation\nperformance. We follow the positional encoding de\ufb01nition in [4].\nWe then apply multiple Transformer layers. Inside each Trans-\nformer layer g(.), we \ufb01rst apply layer normalization, followed by\nmulti-head attention (MHA):\nz\u2032\u2032= MultiHeadAttention(LayerNorm(z\u2032)). (4)\nAs proposed in [4], each attention head computes the scaled dot-\nproduct attention between all the elements of the sequence. The\nTransformer \ufb01nally employs a feed-forward network (FFW), which\nis applied to each position independently:\nz\u2032\u2032\u2032= FeedForward(LayerNorm(z\u2032\u2032+ z\u2032)) +z\u2032\u2032+ z\u2032. (5)\nThe overall transformer block is therefore de\ufb01ned as follows:\nf(z) =gK(z + e) +z, (6)\nwhere gK(.) denotes K layers of transformer layer g(.). We use\nK = Nintra layers for the IntraT, and K = Ninter layers for the\nInterT. As shown in Figure 2 (Bottom) and Eq. (6), we add residual\nconnections across the transformer layers, and across the transformer\narchitecture to improve gradient backpropagation.\n2.4. Decoder\nThe decoder simply uses a transposed convolution layer, with the\nsame stride and kernel size of the encoder. The input to the de-\ncoder is the element-wise multiplication between the mask mk of\nthe source k and the output of the encoder h. The transformation of\nthe decoder can therefore be expressed as follows:\n\u02c6sk = conv1d-transpose(mk \u2217h), (7)\nwhere \u02c6sk \u2208RT denotes the separated source k.\n3. EXPERIMENTAL SETUP\n3.1. Dataset\nWe use the popular WSJ0-2mix and WSJ0-3mix datasets [11] for\nsource separation, where mixtures of two speakers and three speak-\ners are created by randomly mixing utterances in the WSJ0 corpus.\nThe relative levels for the sources are sampled uniformly between 0\ndB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for train-\ning, validation, and test. The training and test sets are created with\ndifferent sets of speakers. The waveforms are sampled at 8 kHz.\n22\nAuthorized licensed use limited to: The George Washington University. Downloaded on October 09,2025 at 02:25:25 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 3106, "end_char_idx": 5852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "fda44cce-f1f8-49f5-80e4-420902d6f118": {"__data__": {"id_": "fda44cce-f1f8-49f5-80e4-420902d6f118", "embedding": null, "metadata": {"page_label": "3", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55dab2a4-db23-477e-b9d2-75b03f270a39", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "562339c2deea81890483c4c0821c9a3ba445f016ca090f501252005ac5b5a9f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e48c1b3-8b9e-4d3b-9309-219cf41fbe98", "node_type": "1", "metadata": {}, "hash": "0e2026d4906402dbe61dbfb3487b7fdc5238c657ac1101ab4cab2cf674bfe32a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "h Norm+Linear Chunking SepFormer PReLU+Linear OverlapAdd FFW+ReLU\nm1\nm2\nh\u2032 h\u2032\u2032 h\u2032\u2032\u2032 h\u2032\u2032\u2032\u2032\nRepeat N times\nh\u2032 IntraTransformer Permute InterTransformer h\u2032\u2032\nRepeat K times\nz\ne\nLayerNorm MHA LayerNorm FFW z\u2032\u2032\u2032 f(z)z\u2032 z\u2032\u2032\nFig. 2. (Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer\narchitecture f(.) that is used both in the IntraTransformer block and in the InterTransformer block.\n3.2. Architecture and Training Details\nThe encoder is based on 256 convolutional \ufb01lters with a kernel size\nof 16 samples and a stride factor of 8 samples. The decoder uses the\nsame kernel size and the stride factors of the encoder.\nIn our best models, the SepFormer masking network processes\nchunks of size C = 250 with a 50 % overlap between them and\nemploys 8 layers of transformers in both IntraT and InterT. The\nIntraT-InterT dual-path processing pipeline is repeatedN = 2times.\nWe used 8 parallel attention heads, and 1024-dimensional positional\nfeed-forward networks within each Transformer layer. The model\nhas a total of 26 million parameters.\nWe explored the use of dynamic mixing (DM) data augmenta-\ntion [23] which consists in on-the-\ufb02y creation of new mixtures from\nsingle speaker sources. In this work we expanded this powerful tech-\nnique by applying also speed perturbation on the sources before mix-\ning them. The speed randomly changes between 95 % slow-down\nand 105 % speed-up.\nWe used the Adam algorithm [25] as optimizer, with a learn-\ning rate of 15e\u22125. After epoch 65 (after epoch 100 with DM), the\nlearning rate is annealed by halving it if we do not observe any im-\nprovement of the validation performance for 3 successive epochs\n(5 epoch for DM). Gradient clipping is employed to limit the L2\nnorm of the gradients to 5. During training, we used a batch size of\n1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) [26]\nvia utterance-level permutation invariant loss [13], with clipping at\n30dB [23]. We used automatic mixed-precision to speed up training.\nThe system is trained for a maximum of 200 epochs. Each epoch\ntakes approximately 1.5 hours on a single NVIDIA V100 GPU with\n32 GB of memory.\n4. RESULTS\n4.1. Results on WSJ0-2mix\nTable 1 compares the performance achieved by the proposed Sep-\nFormer with the best results reported in the literature on the WSJ0-\n2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-\nSNRi) of 22.3 dB and a Signal-to-Distortion Ratio [30] (SDRi) im-\nprovement of 22.4 dB on the test-set with dynamic mixing. When\nusing dynamic mixing, the proposed architecture achieves state-of-\nthe-art performance. The SepFormer outperforms previous systems\nwithout using dynamic mixing except Wavesplit, which uses speaker\nidentity as additional information.\nTable 1. Best results on the WSJ0-2mix dataset (test-set). DM\nstands for dynamic mixing.\nModel SI-SNRi SDRi # Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 2.6M 1\nsudo rm -rf [21] 18.9 n.a.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3166, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8e48c1b3-8b9e-4d3b-9309-219cf41fbe98": {"__data__": {"id_": "8e48c1b3-8b9e-4d3b-9309-219cf41fbe98", "embedding": null, "metadata": {"page_label": "3", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55dab2a4-db23-477e-b9d2-75b03f270a39", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "562339c2deea81890483c4c0821c9a3ba445f016ca090f501252005ac5b5a9f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fda44cce-f1f8-49f5-80e4-420902d6f118", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "8de6068f500373abcfda901a51a18d8a47fdb69f91cbe1aee34ee3bf52b97fe3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DM\nstands for dynamic mixing.\nModel SI-SNRi SDRi # Param Stride\nTasnet [27] 10.8 11.1 n.a 20\nSignPredictionNet [28] 15.3 15.6 55.2M 8\nConvTasnet [15] 15.3 15.6 5.1M 10\nTwo-Step CTN [29] 16.1 n.a. 8.6M 10\nDeepCASA [18] 17.7 18.0 12.8M 1\nFurcaNeXt [19] n.a. 18.4 51.4M n.a.\nDualPathRNN [17] 18.8 19.0 2.6M 1\nsudo rm -rf [21] 18.9 n.a. 2.6M 10\nVSUNOS [20] 20.1 20.4 7.5M 2\nDPTNet* [22] 20.2 20.6 2.6M 1\nWavesplit** [23] 21.0 21.2 29M 1\nWavesplit** + DM [23] 22.2 22.3 29M 1\nSepFormer 20.4 20.5 26M 8\nSepFormer + DM 22.3 22.4 26M 8\n*only SI-SNR and SDR (without improvement) are reported.\n**uses speaker-ids as additional info.\nTable 2. Ablation of the SepFormer on WSJ0-2Mix (validation set).\nSI-SNRi N Nintra Ninter # Heads DFF PosEnc DM\n22.3 2 8 8 8 1024 Yes Yes\n20.5 2 8 8 8 1024 Yes No\n20.4 2 4 4 16 2048 Yes No\n20.2 2 4 4 8 2048 Yes No\n19.9 2 4 4 8 2048 Yes No\n19.8 3 4 4 8 2048 Yes No\n19.4 2 4 4 8 2048 No No\n19.2 2 4 1 8 2048 Yes No\n19.1 2 3 3 8 2048 Yes No\n19.0 2 3 3 8 2048 No No\n4.2. Ablation Study\nHereafter we study the effect of various hyperparameters and data\naugmentation on the performance of the SepFormer using WSJ0-\n2mix dataset. The results are summarized in Table 2. The reported\nperformance in this table is calculated on the validation set.\nWe observe that the number of InterT and IntraT blocks has an\nimportant impact on the performance. The best results are achieved\nwith 8 layers for both blocks replicated two times. We also would\nlike to point out that a respectable performance of 19.2 dB is ob-\ntained even when we use a single layer transformer for the Inter-\n23\nAuthorized licensed use limited to: The George Washington University. Downloaded on October 09,2025 at 02:25:25 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 2834, "end_char_idx": 4581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3d04a8e2-5779-4e18-b079-b4f8f8ef84db": {"__data__": {"id_": "3d04a8e2-5779-4e18-b079-b4f8f8ef84db", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32b065dd-606e-4379-804b-36bb365da6c0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "c787136f2a66c1a49286ed38546eed65bdcf1e5ae543db2b7184df9ce31664ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47500594-559c-42e5-9ccd-6f1a714acc84", "node_type": "1", "metadata": {}, "hash": "80c3c31fc8f451681234eb6369838cdbe2dac65e23def5b55e7b3e9b4576ac21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "0 10 20 30 40\nTraining time (hours)\n8\n10\n12\n14\n16\n18\n20SI-SNRi (dB)\nTraining Speed on WSJ0-2Mix\nSepFormer\nDP-RNN\nDPTNet\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n32.0\n55.0\n95.0\n166.0\n288.0\n501.0miliseconds\nAverage Forward-Pass Time\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\n1.0 2.0 3.0 4.0 5.0\nInput sequence length in seconds\n2.0\n3.0\n6.0\n11.0\n21.0\n40.0GBytes\nMemory Usage\nSepFormer\nDP-RNN\nDPTNet\nWavesplit\nFig. 3. ( Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle & Right)The comparison of\nforward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.\nTable 3. Best results on the WSJ0-3mix dataset.\nModel SI-SNRi SDRi # Param\nConvTasnet [15] 12.7 13.1 5.1M\nDualPathRNN [17] 14.7 n.a 2.6M\nVSUNOS [20] 16.9 n.a 7.5M\nWavesplit [23] 17.3 17.6 29M\nWavesplit [23] + DM 17.8 18.1 29M\nSepformer 17.6 17.9 26M\nSepformer + DM 19.5 19.7 26M\nTransformer. This suggests that the IntraTransformer, and thus lo-\ncal processing, has a greater in\ufb02uence on the performance. It also\nemerges that positional encoding is helpful (e.g. see lines 3 and 5 of\nTable 2). A similar outcome has been observed in [31] for speech\nenhancement. As for the number of attention heads, we observe a\nslight performance difference between 8 and 16 heads. Finally, it\ncan be observed that dynamic mixing helps the performance signi\ufb01-\ncantly.\n4.3. Results on WSJ0-3mix\nTable 3 showcases the best performing models on the WSJ0-3mix\ndataset. SepFormer obtains the state-of-the-art performance with an\nSI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best\narchitecture found for the WSJ0-2mix dataset. The only difference is\nthat the decoder has now three outputs. It is worth noting that on this\ncorpus the SepFormer outperforms all previously proposed systems.\nOur results on WSJ0-2mix and WSJ0-3mix show that it is pos-\nsible to achieve state-of-the-art performance in separation with an\nRNN-free Transformer-based model. The big advantage of Sep-\nFormer over RNN-based systems like [17,20,22] is the possibility to\nparallelize the computations over different time steps. This leads to\nfaster training and inference, as described in the following section.\n4.4. Speed and Memory Comparison\nWe now compare the training and inference speed of our model with\nDPRNN [17] and DPTNet [22]. Figure 3 (left) shows the training\ncurves of the aforementioned models on the WSJ0-2mix dataset.\nWe plot the performance achieved on the validation set in the \ufb01rst\n48 hours of training versus the wall-clock time. For a fair com-\nparison, we used the same machine with the same GPU (a single\nNVIDIA V100-32GB) for all the models. Moreover, all the systems\nare trained with a batch size of 1 and employ automatic mixed pre-\ncision. We observe that the SepFormer is faster than DPRNN and\nDPTNeT. Figure 3 (left), highlights that SepFormer reaches above\n17dB levels only after a full day of training, whereas the DPRNN\nmodel requires two days of training to achieve the same level of per-\nformance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3034, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "47500594-559c-42e5-9ccd-6f1a714acc84": {"__data__": {"id_": "47500594-559c-42e5-9ccd-6f1a714acc84", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32b065dd-606e-4379-804b-36bb365da6c0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "c787136f2a66c1a49286ed38546eed65bdcf1e5ae543db2b7184df9ce31664ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d04a8e2-5779-4e18-b079-b4f8f8ef84db", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "d98b0a6df9b00066a0e803e32c83dcf00b7d540fc17a4618d2be0cbc7ef6de6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 3 (left) shows the training\ncurves of the aforementioned models on the WSJ0-2mix dataset.\nWe plot the performance achieved on the validation set in the \ufb01rst\n48 hours of training versus the wall-clock time. For a fair com-\nparison, we used the same machine with the same GPU (a single\nNVIDIA V100-32GB) for all the models. Moreover, all the systems\nare trained with a batch size of 1 and employ automatic mixed pre-\ncision. We observe that the SepFormer is faster than DPRNN and\nDPTNeT. Figure 3 (left), highlights that SepFormer reaches above\n17dB levels only after a full day of training, whereas the DPRNN\nmodel requires two days of training to achieve the same level of per-\nformance.\nFigure 3 (middle&right) compares the average computation time\n(in ms) and the total memory allocation (in GB) during inference\nwhen single precision is used. We analyze the speed of our best\nmodel for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare\nour models against DP-RNN, DPTNeT, and Wavesplit. All the mod-\nels are stored in the same NVIDIA RTX8000-48GB GPU and we\nperformed this analysis using the PyTorch pro\ufb01ler [32]. For Waves-\nplit we used the implementation in [33].\nFrom this analysis, it emerges that the SepFormer is not only\nfaster but also less memory demanding than DPTNet, DPRNN, and\nWavesplit. We observed the same behavior using the CPU for infer-\nence also. Such a level of computational ef\ufb01ciency is achieved even\nthough the proposed SepFormer employs more parameters than the\nother RNN-based methods (see Table 1). This is not only due to the\nsuperior parallelization capabilities of the proposed model, but also\nbecause the best performance is achieved with a stride factor of 8\nsamples, against a stride of 1 for DPRNN and DPTNet. Increasing\nthe stride of the encoder results in downsampling the input sequence,\nand therefore the model processes less data. In [17], the authors\nshowed that the DPRNN performance degrades when increasing the\nstride factor. The SepFormer, instead, reaches competitive results\neven with a relatively large stride, leading to the aforementioned\nspeed and memory advantages.\n5. CONCLUSIONS\nIn this paper, we proposed a novel neural model for speech sepa-\nration called SepFormer (Separation Transformer). The SepFormer\nis an RNN-free architecture that employs a masking network com-\nposed of transformers only. The masking network learns both short\nand long-term dependencies using a multi-scale approach. Our re-\nsults, reported on the WSJ0-2mix and WSJ0-3mix datasets, high-\nlight that we can reach state-of-the-art performances in source sep-\naration without using RNNs in the network design. This way, com-\nputations over different time-steps can be parallelized. Moreover,\nour model achieves a competitive performance even when subsam-\npling the encoded representation by a factor of 8. These two prop-\nerties lead to a signi\ufb01cant speed-up at training/inference time and\na drastic reduction of memory usage, especially when compared to\nrecent models such as DPRNN, DPTNet, and Wavesplit. As future\nwork, we would like to explore different transformer architectures\nthat could potentially further improve performance, speed, and mem-\nory usage.\n24\nAuthorized licensed use limited to: The George Washington University. Downloaded on October 09,2025 at 02:25:25 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 2340, "end_char_idx": 5697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8221cb0-6595-4da5-89c3-ea10bbc7ebb9": {"__data__": {"id_": "b8221cb0-6595-4da5-89c3-ea10bbc7ebb9", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bd63a9c-9ede-418e-ba67-49f22529d333", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "a6055a76ed20a3ff1a9d5a7b7ee78743efb462f533b8eed639c6b8a00e0076aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d5d3f5d-5663-4b34-b8ae-6e661a807388", "node_type": "1", "metadata": {}, "hash": "32fe6f1ab9088a60d6ed529af49a29488b6450d0676bdd711f0a5148ac985270", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. REFERENCES\n[1] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d\nNeural Computation, vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.\n[2] K. Cho, B. van Merri \u00a8enboer, D. Bahdanau, and Y . Bengio, \u201cOn\nthe properties of neural machine translation: Encoder\u2013decoder\napproaches,\u201d in Proc. of SSST, 2014, pp. 103\u2013111.\n[3] M. Ravanelli, P. Brakel, M. Omologo, and Y . Bengio, \u201cLight\ngated recurrent units for speech recognition,\u201d IEEE Transac-\ntions on Emerging Topics in Computational Intelligence, vol.\n2, no. 2, pp. 92\u2013102, April 2018.\n[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all\nyou need,\u201d CoRR, vol. abs/1706.03762, 2017.\n[5] G. Kerg, B. Kanuparthi, A. Goyal, K. Goyette, Y . Bengio, and\nG. Lajoie, \u201cUntangling tradeoffs between recurrence and self-\nattention in neural networks,\u201d CoRR, vol. abs/2006.09471,\n2020.\n[6] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Yamamoto, X. Wang, S. Watan-\nabe, T. Yoshimura, and W. Zhang, \u201cA comparative study on\ntransformer vs rnn in speech applications,\u201d in Proc. of ASRU,\n2019, pp. 449\u2013456.\n[7] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, \u201cNeural speech\nsynthesis with transformer network,\u201d in Proc. of AAAI, 2019,\npp. 6706\u20136713.\n[8] J. Kim, M. El-Khamy, and J. Lee, \u201cT-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement,\u201d in\nProc. of ICASSP, 2020, pp. 6649\u20136653.\n[9] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, \u201cDis-\ncriminative neural clustering for speaker diarisation,\u201d CoRR,\nvol. abs/1910.09703, 2019.\n[10] X. Chang, W. Zhang, Y . Qian, J. Le Roux, and S. Watan-\nabe, \u201cEnd-to-end multi-speaker speech recognition with trans-\nformer,\u201d in Proc. of ICASSP, 2020, pp. 6134\u20136138.\n[11] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, \u201cDeep\nclustering: Discriminative embeddings for segmentation and\nseparation,\u201d in Proc. of ICASSP, 2016, pp. 31\u201335.\n[12] D. Yu, M. Kolb\u00e6k, Z. Tan, and J. Jensen, \u201cPermutation in-\nvariant training of deep models for speaker-independent multi-\ntalker speech separation,\u201d in Proc. of ICASSP, 2017, pp. 241\u2013\n245.\n[13] M. Kolb\u00e6k, D. Yu, Z.-H. Tan, and J. Jensen, \u201cMultitalker\nspeech separation with utterance-level permutation invariant\ntraining of deep recurrent neural networks,\u201dIEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, vol. 25,\nno. 10, pp. 1901\u20131913, 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\n\u201cEnd-to-end source separation with adaptive front-ends,\u201d in\nProc. of ACSSC, 2018, pp. 684\u2013688.\n[15] Y .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "4d5d3f5d-5663-4b34-b8ae-6e661a807388": {"__data__": {"id_": "4d5d3f5d-5663-4b34-b8ae-6e661a807388", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bd63a9c-9ede-418e-ba67-49f22529d333", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "a6055a76ed20a3ff1a9d5a7b7ee78743efb462f533b8eed639c6b8a00e0076aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8221cb0-6595-4da5-89c3-ea10bbc7ebb9", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "09b74506356f857871e3dc9c0a539d43db44770ba3cba4d542626cf3d1834bba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ed5e9ae-e2f6-46d3-8637-3ff86951828e", "node_type": "1", "metadata": {}, "hash": "54098e11f444b536f7bb5b1ce06e8f438b3d3578b21357584de341f8eea8ec3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of ICASSP, 2017, pp. 241\u2013\n245.\n[13] M. Kolb\u00e6k, D. Yu, Z.-H. Tan, and J. Jensen, \u201cMultitalker\nspeech separation with utterance-level permutation invariant\ntraining of deep recurrent neural networks,\u201dIEEE/ACM Trans-\nactions on Audio, Speech, and Language Processing, vol. 25,\nno. 10, pp. 1901\u20131913, 2017.\n[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\n\u201cEnd-to-end source separation with adaptive front-ends,\u201d in\nProc. of ACSSC, 2018, pp. 684\u2013688.\n[15] Y . Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing Ideal\nTime\u2013Frequency Magnitude Masking for Speech Separation,\u201d\nvol. 27, no. 8, pp. 1256\u20131266, Aug. 2019.\n[16] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\n\u201cDeep learning for monoaural source separation,\u201d in Proc. of\nICASSP, 2014, pp. 1562\u20131566.\n[17] Y . Luo, Z. Chen, and T. Yoshioka, \u201cDual-path rnn: ef\ufb01-\ncient long sequence modeling for time-domain single-channel\nspeech separation,\u201d in Proc. of ICASSP, 2020, pp. 46\u201350.\n[18] Y . Liu and D. Wang, \u201cDivide and conquer: A deep casa\napproach to talker-independent monaural speaker separation,\u201d\nIEEE/ACM Transactions on audio, speech, and language pro-\ncessing, vol. 27, no. 12, 2019.\n[19] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, \u201cFurcanext:\nEnd-to-end monaural speech separation with dynamic gated di-\nlated temporal convolutional networks,\u201d in MultiMedia Mod-\neling, 2020, pp. 653\u2013665.\n[20] E. Nachmani, Y . Adi, and L. Wolf, \u201cV oice separation with\nan unknown number of multiple speakers,\u201d ICML, pp. 7164\u2013\n7175, 2020.\n[21] E. Tzinis, Z. Wang, and P. Smaragdis, \u201cSudo rm -rf: Ef\ufb01cient\nnetworks for universal audio source separation,\u201d in MLSP,\n2020, pp. 1\u20136.\n[22] J. Chen, Q. Mao, and D. Liu, \u201cDual-Path Transformer\nNetwork: Direct Context-Aware Modeling for End-to-End\nMonaural Speech Separation,\u201d in Proc. of Interspeech 2020,\n2020, pp. 2642\u20132646.\n[23] N. Zeghidour and D. Grangier, \u201cWavesplit: End-to-end\nspeech separation by speaker clustering,\u201d arXiv preprint\narXiv:2002.08933, 2020.\n[24] L. J. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d\nCoRR, vol. abs/1607.06450, 2016.\n[25] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[26] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSdr\u2013\nhalf-baked or well done?,\u201d in Proc. of ICASSP. IEEE, 2019,\npp. 626\u2013630.\n[27] Y . Luo and N. Mesgarani, \u201cTasNet: time-domain audio separa-\ntion network for real-time, single-channel speech separation,\u201d\nCoRR, vol. abs/1711.00541, 2017.\n[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, \u201cDeep learning\nbased phase reconstruction for speaker separation: A trigono-\nmetric perspective,\u201d in Proc. of ICASSP, 2019, pp. 71\u201375.", "mimetype": "text/plain", "start_char_idx": 2125, "end_char_idx": 4804, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9ed5e9ae-e2f6-46d3-8637-3ff86951828e": {"__data__": {"id_": "9ed5e9ae-e2f6-46d3-8637-3ff86951828e", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6bd63a9c-9ede-418e-ba67-49f22529d333", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "a6055a76ed20a3ff1a9d5a7b7ee78743efb462f533b8eed639c6b8a00e0076aa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d5d3f5d-5663-4b34-b8ae-6e661a807388", "node_type": "1", "metadata": {"page_label": "5", "file_name": "Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_path": "D:\\Github\\WiiDcos\\Data\\Attention_Is_All_You_Need_In_Speech_Separation.pdf", "file_type": "application/pdf", "file_size": 970218, "creation_date": "2025-10-09", "last_modified_date": "2025-10-09"}, "hash": "5afdd16a2f45f4923744e94e2c1adfe9d447ce10a65bb807cafd0b14d37cfe33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[26] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSdr\u2013\nhalf-baked or well done?,\u201d in Proc. of ICASSP. IEEE, 2019,\npp. 626\u2013630.\n[27] Y . Luo and N. Mesgarani, \u201cTasNet: time-domain audio separa-\ntion network for real-time, single-channel speech separation,\u201d\nCoRR, vol. abs/1711.00541, 2017.\n[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, \u201cDeep learning\nbased phase reconstruction for speaker separation: A trigono-\nmetric perspective,\u201d in Proc. of ICASSP, 2019, pp. 71\u201375.\n[29] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and\nP. Smaragdis, \u201cTwo-step sound source separation: Training on\nlearned latent targets,\u201d in Proc. of ICASSP, 2020, pp. 31\u201335.\n[30] E. Vincent, R. Gribonval, and C F \u00b4evotte, \u201cPerformance mea-\nsurement in blind audio source separation,\u201d IEEE transactions\non audio, speech, and language processing, vol. 14, no. 4, pp.\n1462\u20131469, 2006.\n[31] J. Kim, M. El-Khamy, and J. Lee, \u201cT-gsa: Transformer with\ngaussian-weighted self-attention for speech enhancement,\u201d in\nProc. of ICASSP, 2020, pp. 6649\u20136653.\n[32] Pytorch, \u201cPro\ufb01ler,\u201d https://pytorch.org/\ntutorials/recipes/recipes/profiler.html,\n2020, Accessed: 2020-10-21.\n[33] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzi-\nnis, J. Heitkaemper, M. Olvera, F.-R. St \u00a8oter, M. Hu, J. M.\nMart\u0131n-Do\u02dcnas, D. Ditter, A. Frank, A. Deleforge, and E. Vin-\ncent, \u201cAsteroid: the PyTorch-based audio source separation\ntoolkit for researchers,\u201d in Proc. of Interspeech, 2020, pp.\n2637\u20132641.\n25\nAuthorized licensed use limited to: The George Washington University. Downloaded on October 09,2025 at 02:25:25 UTC from IEEE Xplore.  Restrictions apply.", "mimetype": "text/plain", "start_char_idx": 4324, "end_char_idx": 5952, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}